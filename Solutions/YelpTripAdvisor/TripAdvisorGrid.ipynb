{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_data = open(\"data/data_cleaned_head1000/textfeatures_head1000\", \"r\")\n",
    "target_data = open(\"data/data_cleaned_head1000/starstarget_head1000\", \"r\")\n",
    "#features_data = open(\"data/data_cleaned/textfeatures0\", \"r\")\n",
    "#target_data = open(\"data/data_cleaned/starstarget0\", \"r\")\n",
    "text = [line for line in features_data]\n",
    "target = [int(line.split()[0]) for line in target_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target contains number from 1 to 5, according to the rating (stars). In order to classify the commment in \"positive\" (+1) or \"negative\" (0), we convey to choose positive if stars are grater or equal to 3 and negative elsewhere.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = [ 1 if target[i] >=3 else 0 for i in range(len(target ))]\n",
    "dataset = [[text[i], target[i]] for i in range(len(target))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebalancing the Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a simple counting we realize that the two classes have completly different sizes. There are much more positive comments than negative ones. The ratio is about $1/3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_pos = target.count(1)\n",
    "target_neg = target.count(0)\n",
    "ratio = float(target_neg) / target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rebalance(dataset_to_reb, length, ratio): \n",
    "    reb_dataset = []\n",
    "    for i in range(length):\n",
    "        if dataset_to_reb[i][1] == 0:\n",
    "            reb_dataset.append(dataset_to_reb[i])\n",
    "        else:\n",
    "            rnd = random.random()\n",
    "            if rnd < ratio:\n",
    "                reb_dataset.append(dataset_to_reb[i])\n",
    "    return reb_dataset\n",
    "\n",
    "reb_dataset = rebalance(dataset, len(target), ratio)\n",
    "\n",
    "reb_text = [ tx for [tx,tg] in reb_dataset]\n",
    "reb_target = [ tg for [tx,tg] in reb_dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First selection of stop_words\n",
    "\n",
    "default_stop_words = ['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'not', 'fifty', 'four', 'own', 'through', 'yourselves', 'go', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'your', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once']\n",
    "list_word_to_remove = ['not']\n",
    "my_stop_words = [word for word in default_stop_words if word not in list_word_to_remove]\n",
    "\n",
    "my_stop_words.append('ve') \n",
    "my_stop_words.append('ll')\n",
    "my_stop_words.append('got')\n",
    "my_stop_words.append('know')\n",
    "my_stop_words.append('15')\n",
    "my_stop_words.append('30')\n",
    "my_stop_words.append('20')\n",
    "my_stop_words.append('50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yak52/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reb_text, reb_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industrialize your model: Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 ms, sys: 14 ms, total: 31.2 ms\n",
      "Wall time: 46.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn import pipeline\n",
    "\n",
    "\n",
    "\n",
    "# Pipeline for Decision Tree\n",
    "def my_pipeline_dt(thres):\n",
    "    nsw = np.loadtxt(\"data/stop_words/my_stop_words_thr_%.2f.txt\" %thres, dtype=str)\n",
    "    pipe = pipeline.Pipeline([\n",
    "                            ('count_vectorizer', CountVectorizer(stop_words = list(nsw), max_df = 0.9, min_df = 10)),\n",
    "                            #('count_vectorizer', CountVectorizer(stop_words = my_stop_words, max_df = 0.9, min_df = 10)),\n",
    "                            ('tf_idf',TfidfTransformer()),\n",
    "                            ('model',DecisionTreeClassifier(criterion='gini', \n",
    "                                                            splitter='best', \n",
    "                                                            max_depth=10, \n",
    "                                                            min_samples_split=2, \n",
    "                                                            min_samples_leaf=1, \n",
    "                                                            min_weight_fraction_leaf=0.0, \n",
    "                                                            max_features=None, \n",
    "                                                            random_state=1, \n",
    "                                                            max_leaf_nodes=None, \n",
    "                                                            min_impurity_split=1e-07, \n",
    "                                                            class_weight=None, \n",
    "                                                            presort=False))\n",
    "                         ])\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.890080 \t 0.702128 \t\n"
     ]
    }
   ],
   "source": [
    "mypipe = my_pipeline_dt(0.5).fit(X_train, y_train)\n",
    "score_train_pipe = mypipe.score(X_train, y_train)\n",
    "score_test_pipe = mypipe.score(X_test, y_test)\n",
    "print \"%f \\t %f \\t\" %(score_train_pipe, score_test_pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.97 ms, sys: 4.3 ms, total: 11.3 ms\n",
      "Wall time: 15.1 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import pipeline\n",
    "\n",
    "\n",
    "def my_pipeline_rf(thres):\n",
    "    nsw = np.loadtxt(\"data/stop_words/my_stop_words_thr_%.2f.txt\" %thres, dtype=str)\n",
    "    pipe = pipeline.Pipeline([\n",
    "                            ('count_vectorizer', CountVectorizer(stop_words = list(nsw), max_df = 0.9, min_df = 10)),\n",
    "                            #('count_vectorizer', CountVectorizer(stop_words = my_stop_words, max_df = 0.9, min_df = 10)),\n",
    "                            ('tf_idf',TfidfTransformer()),\n",
    "                            ('model',RandomForestClassifier(n_estimators=200, \n",
    "                                                               criterion='gini',\n",
    "                                                               max_depth=2, \n",
    "                                                               min_samples_split=2, \n",
    "                                                               min_samples_leaf=1, \n",
    "                                                               min_weight_fraction_leaf=0.0, \n",
    "                                                               max_features='auto', \n",
    "                                                               max_leaf_nodes=None, \n",
    "                                                               min_impurity_split=1e-07, \n",
    "                                                               bootstrap=True, \n",
    "                                                               oob_score=False, \n",
    "                                                               n_jobs=1, \n",
    "                                                               random_state=None, \n",
    "                                                               verbose=0, \n",
    "                                                               warm_start=False, \n",
    "                                                               class_weight=None))\n",
    "                         ])\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Classifier\n",
    "\n",
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def my_pipeline_AdaBoost(thres):\n",
    "    nsw = np.loadtxt(\"data/stop_words/my_stop_words_thr_%.2f.txt\" %thres, dtype=str)\n",
    "    pipe = pipeline.Pipeline([\n",
    "                            ('count_vectorizer', CountVectorizer(stop_words = list(nsw), max_df = 0.9, min_df = 10)),\n",
    "                            #('count_vectorizer', CountVectorizer(stop_words = my_stop_words, max_df = 0.9, min_df = 10)),\n",
    "                            ('tf_idf',TfidfTransformer()),\n",
    "                            ('model',AdaBoostClassifier(base_estimator=None, \n",
    "                                                           n_estimators=50, \n",
    "                                                           learning_rate=1.0, \n",
    "                                                           algorithm='SAMME.R', \n",
    "                                                           random_state=None))\n",
    "                         ])\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model: Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def my_pipeline_logreg(thres):\n",
    "    nsw = np.loadtxt(\"data/stop_words/my_stop_words_thr_%.2f.txt\" %thres, dtype=str)\n",
    "    pipe = pipeline.Pipeline([\n",
    "                            ('count_vectorizer', CountVectorizer(stop_words = list(nsw), max_df = 0.9, min_df = 10)),\n",
    "                            #('count_vectorizer', CountVectorizer(stop_words = my_stop_words, max_df = 0.9, min_df = 10)),\n",
    "                            ('tf_idf',TfidfTransformer()),\n",
    "                            ('model',LogisticRegression(penalty='l2', \n",
    "                                                           dual=False, \n",
    "                                                           tol=0.0001, \n",
    "                                                           C=1.0, \n",
    "                                                           fit_intercept=True, \n",
    "                                                           intercept_scaling=1, \n",
    "                                                           class_weight=None, \n",
    "                                                           random_state=None, \n",
    "                                                           solver='liblinear', \n",
    "                                                           max_iter=100,\n",
    "                                                           multi_class='ovr',\n",
    "                                                           verbose=0, \n",
    "                                                           warm_start=False, \n",
    "                                                           n_jobs=1))\n",
    "                         ])\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Optimisation: Grid Search for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from pandas import DataFrame\n",
    "\n",
    "pipe = my_pipeline_rf(0.5)\n",
    "\n",
    "gs = GridSearchCV(\n",
    "                    pipe,\n",
    "                    {\n",
    "                        \"count_vectorizer__min_df\": range(1,10)+range(10,30,5),\n",
    "                        \"model__max_depth\": range(1,20)+range(30,100,10)\n",
    "                    },\n",
    "                    cv=2,  # 5-fold cross validation\n",
    "                    n_jobs=16,  # run each hyperparameter in one of two parallel jobs\n",
    "                    scoring=\"accuracy\" # what could happen selecting \"precision\" as scoring measure?\n",
    "                )\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "np.save(\"rf_grid_results.npy\", gs.cv_results_)\n",
    "np.save(\"rf_grid_bestparameters.npy\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Optimisation: Grid Search for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count_vectorizer__min_df': 2, 'model__max_depth': 80}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_count_vectorizer__min_df</th>\n",
       "      <th>param_model__max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.068869</td>\n",
       "      <td>0.186368</td>\n",
       "      <td>0.584450</td>\n",
       "      <td>0.702432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{u'count_vectorizer__min_df': 1, u'model__max_...</td>\n",
       "      <td>337</td>\n",
       "      <td>0.614973</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.553763</td>\n",
       "      <td>0.695187</td>\n",
       "      <td>0.030202</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.030605</td>\n",
       "      <td>0.007245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.036088</td>\n",
       "      <td>0.173914</td>\n",
       "      <td>0.635389</td>\n",
       "      <td>0.801622</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{u'count_vectorizer__min_df': 1, u'model__max_...</td>\n",
       "      <td>319</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.796791</td>\n",
       "      <td>0.013968</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.011701</td>\n",
       "      <td>0.004830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.075050</td>\n",
       "      <td>0.194080</td>\n",
       "      <td>0.683646</td>\n",
       "      <td>0.868639</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>{u'count_vectorizer__min_df': 1, u'model__max_...</td>\n",
       "      <td>222</td>\n",
       "      <td>0.700535</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.866310</td>\n",
       "      <td>0.068205</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>0.016934</td>\n",
       "      <td>0.002329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.138940</td>\n",
       "      <td>0.187187</td>\n",
       "      <td>0.672922</td>\n",
       "      <td>0.882080</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>{u'count_vectorizer__min_df': 1, u'model__max_...</td>\n",
       "      <td>253</td>\n",
       "      <td>0.689840</td>\n",
       "      <td>0.897849</td>\n",
       "      <td>0.655914</td>\n",
       "      <td>0.866310</td>\n",
       "      <td>0.064726</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.015770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.041751</td>\n",
       "      <td>0.192088</td>\n",
       "      <td>0.686327</td>\n",
       "      <td>0.884653</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>{u'count_vectorizer__min_df': 1, u'model__max_...</td>\n",
       "      <td>209</td>\n",
       "      <td>0.700535</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.672043</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.005761</td>\n",
       "      <td>0.014246</td>\n",
       "      <td>0.024438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.020226</td>\n",
       "      <td>0.182668</td>\n",
       "      <td>0.686327</td>\n",
       "      <td>0.914180</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>{u'count_vectorizer__min_df': 1, u'model__max_...</td>\n",
       "      <td>209</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.688172</td>\n",
       "      <td>0.925134</td>\n",
       "      <td>0.010428</td>\n",
       "      <td>0.006972</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.010954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.040145</td>\n",
       "      <td>0.179445</td>\n",
       "      <td>0.715818</td>\n",
       "      <td>0.930294</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>{u'count_vectorizer__min_df': 1, u'model__max_...</td>\n",
       "      <td>61</td>\n",
       "      <td>0.721925</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.930481</td>\n",
       "      <td>0.014741</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.006124</td>\n",
       "      <td>0.000187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.068545</td>\n",
       "      <td>0.180240</td>\n",
       "      <td>0.680965</td>\n",
       "      <td>0.946323</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>{u'count_vectorizer__min_df': 1, u'model__max_...</td>\n",
       "      <td>239</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.967914</td>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>0.021592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.035372</td>\n",
       "      <td>0.180315</td>\n",
       "      <td>0.721180</td>\n",
       "      <td>0.959778</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>{u'count_vectorizer__min_df': 1, u'model__max_...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.736559</td>\n",
       "      <td>0.962567</td>\n",
       "      <td>0.005584</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.015338</td>\n",
       "      <td>0.002789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.035758</td>\n",
       "      <td>0.178486</td>\n",
       "      <td>0.707775</td>\n",
       "      <td>0.957090</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{u'count_vectorizer__min_df': 1, u'model__max_...</td>\n",
       "      <td>114</td>\n",
       "      <td>0.721925</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.962567</td>\n",
       "      <td>0.021173</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.014188</td>\n",
       "      <td>0.005477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       1.068869         0.186368         0.584450          0.702432   \n",
       "1       1.036088         0.173914         0.635389          0.801622   \n",
       "2       1.075050         0.194080         0.683646          0.868639   \n",
       "3       1.138940         0.187187         0.672922          0.882080   \n",
       "4       1.041751         0.192088         0.686327          0.884653   \n",
       "5       1.020226         0.182668         0.686327          0.914180   \n",
       "6       1.040145         0.179445         0.715818          0.930294   \n",
       "7       1.068545         0.180240         0.680965          0.946323   \n",
       "8       1.035372         0.180315         0.721180          0.959778   \n",
       "9       1.035758         0.178486         0.707775          0.957090   \n",
       "\n",
       "  param_count_vectorizer__min_df param_model__max_depth  \\\n",
       "0                              1                      1   \n",
       "1                              1                      2   \n",
       "2                              1                      3   \n",
       "3                              1                      4   \n",
       "4                              1                      5   \n",
       "5                              1                      6   \n",
       "6                              1                      7   \n",
       "7                              1                      8   \n",
       "8                              1                      9   \n",
       "9                              1                     10   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {u'count_vectorizer__min_df': 1, u'model__max_...              337   \n",
       "1  {u'count_vectorizer__min_df': 1, u'model__max_...              319   \n",
       "2  {u'count_vectorizer__min_df': 1, u'model__max_...              222   \n",
       "3  {u'count_vectorizer__min_df': 1, u'model__max_...              253   \n",
       "4  {u'count_vectorizer__min_df': 1, u'model__max_...              209   \n",
       "5  {u'count_vectorizer__min_df': 1, u'model__max_...              209   \n",
       "6  {u'count_vectorizer__min_df': 1, u'model__max_...               61   \n",
       "7  {u'count_vectorizer__min_df': 1, u'model__max_...              239   \n",
       "8  {u'count_vectorizer__min_df': 1, u'model__max_...               42   \n",
       "9  {u'count_vectorizer__min_df': 1, u'model__max_...              114   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.614973            0.709677           0.553763   \n",
       "1           0.647059            0.806452           0.623656   \n",
       "2           0.700535            0.870968           0.666667   \n",
       "3           0.689840            0.897849           0.655914   \n",
       "4           0.700535            0.860215           0.672043   \n",
       "5           0.684492            0.903226           0.688172   \n",
       "6           0.721925            0.930108           0.709677   \n",
       "7           0.684492            0.924731           0.677419   \n",
       "8           0.705882            0.956989           0.736559   \n",
       "9           0.721925            0.951613           0.693548   \n",
       "\n",
       "   split1_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0            0.695187      0.030202        0.001224        0.030605   \n",
       "1            0.796791      0.013968        0.000719        0.011701   \n",
       "2            0.866310      0.068205        0.007957        0.016934   \n",
       "3            0.866310      0.064726        0.003754        0.016963   \n",
       "4            0.909091      0.001550        0.005761        0.014246   \n",
       "5            0.925134      0.010428        0.006972        0.001840   \n",
       "6            0.930481      0.014741        0.000986        0.006124   \n",
       "7            0.967914      0.010634        0.002979        0.003536   \n",
       "8            0.962567      0.005584        0.001026        0.015338   \n",
       "9            0.962567      0.021173        0.001817        0.014188   \n",
       "\n",
       "   std_train_score  \n",
       "0         0.007245  \n",
       "1         0.004830  \n",
       "2         0.002329  \n",
       "3         0.015770  \n",
       "4         0.024438  \n",
       "5         0.010954  \n",
       "6         0.000187  \n",
       "7         0.021592  \n",
       "8         0.002789  \n",
       "9         0.005477  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from pandas import DataFrame\n",
    "\n",
    "pipe = my_pipeline_lr(0.5)\n",
    "\n",
    "gs = GridSearchCV(\n",
    "                    pipe,\n",
    "                    {\n",
    "                        \"count_vectorizer__min_df\": range(1,10)+range(10,30,5),\n",
    "                        \"model__C\": [0.1, 0.5, 1.0, 3.0, 5.0, 10, 20, 50, 80]\n",
    "                    },\n",
    "                    cv=2,  # 5-fold cross validation\n",
    "                    n_jobs=16,  # run each hyperparameter in one of two parallel jobs\n",
    "                    scoring=\"accuracy\" # what could happen selecting \"precision\" as scoring measure?\n",
    "                )\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "np.save(\"lr_grid_results.npy\", gs.cv_results_)\n",
    "np.save(\"lr_grid_bestparameters.npy\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(range(1,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(1,20)+range(30,100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
