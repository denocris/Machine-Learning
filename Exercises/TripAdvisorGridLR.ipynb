{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features_data = open(\"data/data_cleaned_head1000/textfeatures_head1000\", \"r\")\n",
    "#target_data = open(\"data/data_cleaned_head1000/starstarget_head1000\", \"r\")\n",
    "features_data = open(\"data/data_cleaned/textfeatures0\", \"r\")\n",
    "target_data = open(\"data/data_cleaned/starstarget0\", \"r\")\n",
    "text = [line for line in features_data]\n",
    "target = [int(line.split()[0]) for line in target_data]\n",
    "\n",
    "target = [ 1 if target[i] >=3 else 0 for i in range(len(target ))]\n",
    "dataset = [[text[i], target[i]] for i in range(len(target))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebalancing the Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_pos = target.count(1)\n",
    "target_neg = target.count(0)\n",
    "ratio = float(target_neg) / target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rebalance(dataset_to_reb, length, ratio): \n",
    "    reb_dataset = []\n",
    "    for i in range(length):\n",
    "        if dataset_to_reb[i][1] == 0:\n",
    "            reb_dataset.append(dataset_to_reb[i])\n",
    "        else:\n",
    "            rnd = random.random()\n",
    "            if rnd < ratio:\n",
    "                reb_dataset.append(dataset_to_reb[i])\n",
    "    return reb_dataset\n",
    "\n",
    "reb_dataset = rebalance(dataset, len(target), ratio)\n",
    "\n",
    "reb_text = [ tx for [tx,tg] in reb_dataset]\n",
    "reb_target = [ tg for [tx,tg] in reb_dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First selection of stop_words\n",
    "\n",
    "default_stop_words = ['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'not', 'fifty', 'four', 'own', 'through', 'yourselves', 'go', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'your', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once']\n",
    "list_word_to_remove = ['not']\n",
    "my_stop_words = [word for word in default_stop_words if word not in list_word_to_remove]\n",
    "\n",
    "my_stop_words.append('ve') \n",
    "my_stop_words.append('ll')\n",
    "my_stop_words.append('got')\n",
    "my_stop_words.append('know')\n",
    "my_stop_words.append('15')\n",
    "my_stop_words.append('30')\n",
    "my_stop_words.append('20')\n",
    "my_stop_words.append('50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yak52/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reb_text, reb_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model: Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64 µs, sys: 322 µs, total: 386 µs\n",
      "Wall time: 474 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import pipeline\n",
    "\n",
    "\n",
    "def my_pipeline_logreg(thres):\n",
    "    nsw = np.loadtxt(\"data/stop_words/my_stop_words_thr_%.2f.txt\" %thres, dtype=str)\n",
    "    pipe = pipeline.Pipeline([\n",
    "                            ('count_vectorizer', CountVectorizer(stop_words = list(nsw), max_df = 0.9, min_df = 10)),\n",
    "                            #('count_vectorizer', CountVectorizer(stop_words = my_stop_words, max_df = 0.9, min_df = 10)),\n",
    "                            ('tf_idf',TfidfTransformer()),\n",
    "                            ('model',LogisticRegression(penalty='l2', \n",
    "                                                           dual=False, \n",
    "                                                           tol=0.0001, \n",
    "                                                           C=1.0, \n",
    "                                                           fit_intercept=True, \n",
    "                                                           intercept_scaling=1, \n",
    "                                                           class_weight=None, \n",
    "                                                           random_state=None, \n",
    "                                                           solver='liblinear', \n",
    "                                                           max_iter=100,\n",
    "                                                           multi_class='ovr',\n",
    "                                                           verbose=0, \n",
    "                                                           warm_start=False, \n",
    "                                                           n_jobs=1))\n",
    "                         ])\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Optimisation: Grid Search for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from pandas import DataFrame\n",
    "\n",
    "pipe = my_pipeline_logreg(0.5)\n",
    "\n",
    "gs = GridSearchCV(\n",
    "                    pipe,\n",
    "                    {\n",
    "                        \"count_vectorizer__min_df\": range(1,10)+range(10,30,5),\n",
    "                        \"model__C\": [0.1, 0.5, 1.0, 3.0, 5.0, 10, 20, 50, 80]\n",
    "                    },\n",
    "                    cv=2,  # 5-fold cross validation\n",
    "                    n_jobs=20,  # run each hyperparameter in one of two parallel jobs\n",
    "                    scoring=\"accuracy\" # what could happen selecting \"precision\" as scoring measure?\n",
    "                )\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "np.save(\"lr_grid_results.npy\", gs.cv_results_)\n",
    "np.save(\"lr_grid_bestparameters.npy\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
